{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "____\n",
    "\n",
    "Bellow, you'll find the code that was used to generate the activity of place cells on a linear track. Use the code and the decoding procedure you learned to explore how different features of the data impact our ability to decode position. \n",
    "\n",
    "<font color='teal'> **Q1.1** - Try to use different fractions of the data samples. How does the median error change when the number of available samples gets larger? </font>\n",
    "\n",
    "<font color='teal'> **Q1.2** - How many place cells do we need to reliably decode position? Try to re-do the decoding using only 10, 20, 30,... cells. How does the median error change? </font>\n",
    "\n",
    "<font color='teal'> **Q1.3** - Generate new data changing the firing rate noise. How does this impact decoding? </font>\n",
    "_____\n",
    "\n",
    "**ANS.:**\n",
    "\n",
    "See bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trajectory\n",
    "n_runs = 10 \n",
    "use_stops = False\n",
    "fps = 10\n",
    "track_length = 200\n",
    "bins = np.arange(0, track_length)\n",
    "\n",
    "running_speed_a = np.random.chisquare(10, n_runs)\n",
    "running_speed_b = np.random.chisquare(10, n_runs)\n",
    "\n",
    "stopping_time_a = np.random.chisquare(15, n_runs)\n",
    "stopping_time_b = np.random.chisquare(15, n_runs)\n",
    "\n",
    "x = np.array([])\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "    stop1 = np.zeros( int(stopping_time_a[i] * fps) )\n",
    "    run_length = int( len(bins) * fps / running_speed_a[i] )\n",
    "    run1 = np.linspace(0, float(len(bins) - 1), run_length)\n",
    "    \n",
    "    stop2 = np.ones((int(stopping_time_b[i]*fps),)) * (len(bins) - 1.)\n",
    "    run_length = int( len(bins) * fps / running_speed_b[i] )\n",
    "    run2 = np.linspace(len(bins) - 1, 0, run_length)\n",
    "    \n",
    "    if use_stops:\n",
    "        x = np.concatenate((x, stop1, run1, stop2, run2))\n",
    "    else:\n",
    "         x = np.concatenate((x, run1, run2))\n",
    "\n",
    "t = np.arange(len(x)) / fps\n",
    "\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(t, x, 'k')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_firing_rate = 5   # Peak firing rate, averaged across the population \n",
    "n_cells = 100 \n",
    "\n",
    "pf_centers = np.random.rand(n_cells) * track_length                          # Place field centers\n",
    "pf_size = np.random.gamma(10, size=n_cells)                                  # Place field widths \n",
    "pf_rate = np.random.exponential(scale=average_firing_rate, size=n_cells)     # Peak firing rate for each cell\n",
    "\n",
    "true_firing_rate_maps = np.zeros((n_cells, len(bins)))\n",
    "\n",
    "for i in range(n_cells):\n",
    "    # Gaussian spread firing rate\n",
    "    true_firing_rate_maps[i,:] = pf_rate[i] * np.exp( -(( bins - pf_centers[i])**2 ) / ( 2*pf_size[i]**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.1** - Try to use different fractions of the data samples. How does the median error change when the number of available samples gets larger? </font>\n",
    "\n",
    "To explore the effect of using a different number of samples on the median error, we change the former by changing the `sampling_rate` (10, 100, 1000). What we observe is that the median usually starts in the highest value, quickly decreases to $\\approx 3.6$ and oscillates around that value as the sampling rate increases.\n",
    "\n",
    "This means that after some point, no matter how many samples we collect , we are not necessarily increasing the accuracy of the prediction, rather it plateaus. This feels intuitive given that increasing the sampling rate means only that we increase the number of samples collected each second so the temporal resolution is still the same. After some number of samples, we are effectively just collecting redundant data hence the convergence of the median error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples we collect each second\n",
    "sampling_rate = 100\n",
    "\n",
    "t_sampling = np.arange(0, t[-1], 1 / sampling_rate)\n",
    "\n",
    "# Given our sampling rate, what actual x's do we record\n",
    "x_sampling = np.floor(np.interp(t_sampling, t, x)) \n",
    "print(len(x_sampling))\n",
    "noise_firing_rate = 0.1 \n",
    "spike_times = []\n",
    "bins = np.arange(0, track_length)\n",
    "\n",
    "for i in range(n_cells):\n",
    "\n",
    "    inst_rate = true_firing_rate_maps[i, x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "    spikes_loc = np.random.poisson(inst_rate / sampling_rate)\n",
    "    sp = np.argwhere(spikes_loc) # Find indices of non-zero elements\n",
    "    t_sp = t_sampling[sp]\n",
    "    spike_times.append(t_sp)\n",
    "\n",
    "# Extract the spike counts for each neuron\n",
    "spike_counts = np.asarray([np.histogram(s,t)[0] for s in spike_times])\n",
    "spike_counts = spike_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_decoding(x, t, firing_rate_map, spike_counts, fps, bins):\n",
    "\n",
    "    x, t = x[:-1], t[:-1]\n",
    "    decoded_location = np.zeros(len(x))\n",
    "\n",
    "    for t_bin in tqdm(range(len(t))):\n",
    "\n",
    "        # Check if the time bin has spikes\n",
    "        if sum(spike_counts[t_bin,:]) > 0:\n",
    "            posterior = np.empty(firing_rate_map.shape[-1])\n",
    "\n",
    "            for i in range(len(posterior)):\n",
    "                posterior[i] = sum(poisson.logpmf(spike_counts[t_bin,:], firing_rate_map[:,i] / fps), pow(1, -15))\n",
    "                \n",
    "            decoded_location[t_bin] = bins[np.argmax(posterior)]\n",
    "        \n",
    "        else:\n",
    "            decoded_location[t_bin] = np.nan\n",
    "\n",
    "    return decoded_location\n",
    "\n",
    "def faster_bayes_decoding(firing_rate_map, spike_counts, fps, bins):\n",
    "    epsilon = pow(1, -10)\n",
    "    log_posteriors = spike_counts @ np.log(firing_rate_map + epsilon) - (1/fps) * np.sum(firing_rate_map, axis = 0)\n",
    "    decoded_locations = [bins[np.argmax(P)] for P in log_posteriors]\n",
    "    return decoded_locations\n",
    "\n",
    "'''s1 = time.time()\n",
    "decoded_locations = bayes_decoding(x,t, true_firing_rate_maps, spike_counts, fps,bins)\n",
    "e1 = time.time()\n",
    "\n",
    "s2 = time.time()\n",
    "faster_decoded_locations = faster_bayes_decoding(true_firing_rate_maps, spike_counts, fps,bins)\n",
    "e2 = time.time()\n",
    "\n",
    "print('First one took {} and second took {}'.format(np.round(e1-s1, 3), np.round(e2-s2,3)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting, prediction/decoding\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(decoded_locations, '.', color='indianred', label='Decoded')\n",
    "plt.plot(x[:-1], 'k', label='Ground truth')\n",
    "plt.grid()\n",
    "plt.legend(shadow=True)\n",
    "plt.show()\n",
    "\n",
    "# Error distribution\n",
    "mse = np.sqrt((x[:-1] - decoded_locations)**2)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(mse, bins=300, color='teal', alpha=0.75)\n",
    "plt.axvline(x = np.nanmedian(mse), c='k', label = 'Median = {}'.format(np.round(np.nanmedian(mse), 2)))\n",
    "plt.legend(shadow=True)\n",
    "plt.grid()\n",
    "plt.xlim([0,50])\n",
    "plt.title('Error distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = np.arange(10, 500, 10)\n",
    "medians = np.zeros(len(sr))\n",
    "\n",
    "for s, sampling_rate in enumerate(sr):\n",
    "   \n",
    "    t_sampling = np.arange(0, t[-1], 1 / sampling_rate)   \n",
    "    x_sampling = np.floor(np.interp(t_sampling, t, x)) \n",
    "    noise_firing_rate = 0.1 \n",
    "    spike_times = []\n",
    "    for i in range(n_cells):\n",
    "        inst_rate = true_firing_rate_maps[i, x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "        spikes_loc = np.random.poisson(inst_rate / sampling_rate)\n",
    "        sp = np.argwhere(spikes_loc) \n",
    "        t_sp = t_sampling[sp]\n",
    "        spike_times.append(t_sp)\n",
    "    spike_counts = np.asarray([np.histogram(s,t)[0] for s in spike_times])\n",
    "    spike_counts = spike_counts.T\n",
    "\n",
    "    decoded_locations = faster_bayes_decoding(true_firing_rate_maps, spike_counts, fps, bins)\n",
    "    mse = np.sqrt((x[:-1] - decoded_locations)**2)\n",
    "    medians[s] =  np.nanmedian(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sr, medians,'.-k')\n",
    "plt.grid()\n",
    "plt.xlabel('Sampling rate')\n",
    "plt.ylabel('Median')\n",
    "\n",
    "med = np.mean(medians[10:])\n",
    "plt.axhline(med, linestyle= '--' ,color = 'indianred', label='Avg median (excluding init points)')\n",
    "plt.legend(shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.2** - How many place cells do we need to reliably decode position? Try to re-do the decoding using only 10, 20, 30,... cells. How does the median error change? </font>\n",
    "\n",
    "\n",
    "With increasing number of cells the median of the error decreases. We that the curve approaches the same value as in the previous exercise though it a stabler fashion as it doesn't oscillate as much. The difference from the previous exercise is that further increasing th enumber of cells seems to decreases the median even more but much slower. \n",
    "\n",
    "As such, increasing the number of cells being recorded does improve the accuracy of the prediction but at some point, in a real experiment, we would have to stop as the accuracy we would get wouldn't compensate for the cost of recording more cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_map(n_cells, bins, avg_firing_rate = 5, track_length = 200):\n",
    "    \n",
    "    pf_centers = np.random.rand(n_cells) * track_length                          # Place field centers\n",
    "    pf_size = np.random.gamma(10, size=n_cells)                                  # Place field widths \n",
    "    pf_rate = np.random.exponential(scale=avg_firing_rate, size=n_cells)     # Peak firing rate for each cell\n",
    "\n",
    "    fr_maps = np.zeros((n_cells, len(bins)))\n",
    "\n",
    "    for i in range(n_cells):\n",
    "        # Gaussian spread firing rate\n",
    "        fr_maps[i,:] = pf_rate[i] * np.exp( -(( bins - pf_centers[i])**2 ) / ( 2*pf_size[i]**2 ))\n",
    "\n",
    "    return fr_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cells = np.arange(10, 1001, 25) \n",
    "medians = np.zeros(len(number_of_cells))\n",
    "fps = 10\n",
    "track_length = 200\n",
    "bins = np.arange(0, track_length)\n",
    "sampling_rate = 100\n",
    "\n",
    "for n, n_cells in enumerate(number_of_cells):\n",
    "   \n",
    "    t_sampling = np.arange(0, t[-1], 1 / sampling_rate)   \n",
    "    x_sampling = np.floor(np.interp(t_sampling, t, x)) \n",
    "    noise_firing_rate = 0.1 \n",
    "    spike_times = []\n",
    "\n",
    "    true_firing_rate_maps = fr_map(n_cells, bins)\n",
    "\n",
    "    for i in range(n_cells):\n",
    "\n",
    "        inst_rate = true_firing_rate_maps[i, x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "        spikes_loc = np.random.poisson(inst_rate / sampling_rate)\n",
    "        sp = np.argwhere(spikes_loc) \n",
    "        t_sp = t_sampling[sp]\n",
    "        spike_times.append(t_sp)\n",
    "\n",
    "    spike_counts = np.asarray([np.histogram(s,t)[0] for s in spike_times])\n",
    "    spike_counts = spike_counts.T\n",
    "\n",
    "    decoded_locations = faster_bayes_decoding(true_firing_rate_maps, spike_counts, fps, bins)\n",
    "    mse = np.sqrt((x[:-1] - decoded_locations)**2)\n",
    "    medians[n] =  np.nanmedian(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(number_of_cells, medians,'.-k')\n",
    "plt.grid()\n",
    "plt.xlabel('# Cells')\n",
    "plt.ylabel('Median')\n",
    "\n",
    "plt.axhline(med, linestyle= '--' ,color = 'indianred', label='Avg median - ex.(1.1)')\n",
    "plt.legend(shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.3** - Generate new data changing the firing rate noise. How does this impact decoding? </font>\n",
    "\n",
    "Clearly,it worsens it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = np.arange(0.01, 5, 0.05) \n",
    "medians = np.zeros(len(noises))\n",
    "fps = 10\n",
    "track_length = 200\n",
    "bins = np.arange(0, track_length)\n",
    "sampling_rate, n_cells = 100, 100\n",
    "\n",
    "for n, noise_firing_rate in enumerate(noises):\n",
    "   \n",
    "    t_sampling = np.arange(0, t[-1], 1 / sampling_rate)   \n",
    "    x_sampling = np.floor(np.interp(t_sampling, t, x)) \n",
    "    spike_times = []\n",
    "\n",
    "    true_firing_rate_maps = fr_map(n_cells, bins)\n",
    "\n",
    "    for i in range(n_cells):\n",
    "\n",
    "        inst_rate = true_firing_rate_maps[i, x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "        spikes_loc = np.random.poisson(inst_rate / sampling_rate)\n",
    "        sp = np.argwhere(spikes_loc) \n",
    "        t_sp = t_sampling[sp]\n",
    "        spike_times.append(t_sp)\n",
    "\n",
    "    spike_counts = np.asarray([np.histogram(s,t)[0] for s in spike_times])\n",
    "    spike_counts = spike_counts.T\n",
    "\n",
    "    decoded_locations = faster_bayes_decoding(true_firing_rate_maps, spike_counts, fps, bins)\n",
    "    mse = np.sqrt((x[:-1] - decoded_locations)**2)\n",
    "    medians[n] =  np.nanmedian(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(noises, medians,'.-k')\n",
    "plt.grid()\n",
    "plt.xlabel('Firing rate noise')\n",
    "plt.ylabel('Median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "____\n",
    "\n",
    "In the loop implementation of the decoder, we used the `poison.logpmf(k, mu)` to calculate the log probability of observing $k$ spikes given an average firing rate $\\mu$. This is mathematically equivalent to `np.log(poisson.pmf(k,  mu))`, in which we calculate the probability first and then take th elog.\n",
    "\n",
    "Re-run the decoding with \n",
    "\n",
    "```\n",
    "posterior[i] = sum(np.log(poisson.pmf(spikes_count[t_bin,:],firing_rate_maps[:,i]/fps)+pow(1,-15)))\n",
    "```\n",
    "\n",
    "Do you observe any differences in the results? What do you think this is due to?\n",
    "_____\n",
    "\n",
    "**ANS.:**\n",
    "\n",
    "Though they ultimately yielçd the same result, they arrive at it differently. The `poisson.logpmf()` computes the natural logarithm of the probability mass function of the Poisson distribution directly whereas `np.log(poinsson.pmf())` first calculates the pmf and then takes the natural logarithm\n",
    "\n",
    "The difference between using `poisson.logpmf()` or `np.log(poinsson.pmf())` is evident. The latter is a more unstable or more susceptible to data flutuations than the former. This can be seen by the fact that many locations are predicted as zero. The efficiency of `poisson.logpmf()` arises from the fact that it does not go through the intermediate steps of calculating the two functions separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_decoding_ex2(x, t, firing_rate_map, spike_counts, fps, bins):\n",
    "\n",
    "    x, t = x[:-1], t[:-1]\n",
    "    decoded_location = np.zeros(len(x))\n",
    "\n",
    "    for t_bin in tqdm(range(len(t))):\n",
    "\n",
    "        # Check if the time bin has spikes\n",
    "        if sum(spike_counts[t_bin,:]) > 0:\n",
    "            posterior = np.empty(firing_rate_map.shape[-1])\n",
    "\n",
    "            for i in range(len(posterior)):\n",
    "                posterior[i] = sum(np.log(poisson.pmf(spike_counts[t_bin,:],firing_rate_map[:,i]/fps)+pow(1,-15)))\n",
    "            decoded_location[t_bin] = bins[np.argmax(posterior)]\n",
    "        \n",
    "        else:\n",
    "            decoded_location[t_bin] = np.nan\n",
    "\n",
    "    return decoded_location\n",
    "\n",
    "decoded_locations1 = bayes_decoding(x, t, true_firing_rate_maps, spike_counts, fps, bins)\n",
    "decoded_locations2 = bayes_decoding_ex2(x, t, true_firing_rate_maps, spike_counts, fps, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1 = np.nanmedian(np.sqrt((x[:-1] - decoded_locations1)**2))\n",
    "mse2 = np.nanmedian(np.sqrt((x[:-1] - decoded_locations2)**2))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(decoded_locations1, '.', color='indianred', label='Median of error = {}'.format(np.round(mse1,2)))\n",
    "plt.plot(x[:-1], 'k')\n",
    "plt.legend(shadow=True, loc='upper right')\n",
    "plt.title('poisson.logpmf()')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(decoded_locations2, '.', color='teal', label='Median of error = {}'.format(np.round(mse2, 2)))\n",
    "plt.title('np.log(poisson.pmf())')\n",
    "plt.plot(x[:-1], 'k')\n",
    "plt.legend(shadow=True, loc='upper right')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "____\n",
    "\n",
    "<font color='teal'> **Q1.1** - Estimate the quality of the sequence detection methods we saw in the lesson. How many false positives/negatives does it find? </font>\n",
    "\n",
    "<font color='teal'> **Q1.2** -  Investigate the effect of `noise_x_react` and `noise_t_react` on the false positive/negative rates of the detection procedure.</font>\n",
    "\n",
    "<font color='teal'> **Q1.3** -  What kind of sequence can our methods detect? What kind of activity, despite being sequential, could escape our detection method? Do you have any idea of a different method for sequence detection? </font>\n",
    "_____\n",
    "\n",
    "**ANS.:**\n",
    "\n",
    "See bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.1** - Estimate the quality of the sequence detection methods we saw in the lesson. How many false positives/negatives does it find? </font>\n",
    "\n",
    "Suppose we generated 200 events half of which are sequences and the other half random activity. The measure we use to distinguish between the two is that the slope of the predicted locations. The slope is zero for random activity and non-zero for actuall sequences.\n",
    "\n",
    "In one run of the code in the notebook I found the following:\n",
    "\n",
    "<img src='exercise_3_histograms.png'>\n",
    "\n",
    "Setting a threshold of $\\pm 1$ for the random activity\n",
    "\n",
    "```\n",
    "inds_less = np.where(reactivation_slopes < -1)[0] \n",
    "inds_more = np.where(reactivation_slopes > 1)[0]\n",
    "```\n",
    "\n",
    "I found that out of the 200 events, 186 were predicted as sequences. That is, the number of false positives is much bigger than the number of false negatives. So the quality of the method isn't bery good. This could be caused by the fact that such a small number of cells can still yield some linear structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.2** -  Investigate the effect of `noise_x_react` and `noise_t_react` on the false positive/negative rates of the detection procedure.</font>\n",
    "\n",
    "It doesn't seem to have any effect. I tried `noise_x/y_react = 0, 0.5, 5` and got the following.\n",
    "\n",
    "<img src='noise_react.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'> **Q1.3** -  What kind of sequence can our methods detect? What kind of activity, despite being sequential, could escape our detection method? Do you have any idea of a different method for sequence detection? </font>\n",
    "\n",
    "I would suspect that any kind of curved track couldn't be account with this method. This because the beginning and end of the track would elicit a spike from nearby (if not the same cell) and we could no longer rely on the slopes to distinguish betweensequence and non-sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, none of the results above seem quite right....\n",
    "\n",
    "Before running anything I was expecting to get some false positives and negatives. However, I wasn't expecting to get such a small number of (near) zero slopes. If there's nothing wrong with what I did, there's a great fault in this method, rendering it useless even. Nonetheless, I think that a higher number of non-zero slopes should be higher than what I initially thought because one can fit a staright line on random data quite easily, that is, it is not completely discombobulated to have random dots slightly clustered.\n",
    "\n",
    "I discussed this with the TA but he couldn't give much insight. Is this method actually used?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
